{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nfrom os import listdir\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as Layers\nimport tensorflow.keras.models as Models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(os.listdir(\"../input/osic-pulmonary-fibrosis-progression\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDataFrame = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\ntestDataFrame = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(trainDataFrame.shape)\nprint(\"---------------------------------------------------\")\nprint(testDataFrame.shape)\nprint(\"---------------------------------------------------\")\nprint(trainDataFrame.info())\nprint(\"---------------------------------------------------\")\nprint(testDataFrame.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"The total patient ids are {trainDataFrame['Patient'].count()}\")\nprint(f\"Number of unique ids are {trainDataFrame['Patient'].value_counts().shape[0]} \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDataFrame.drop_duplicates(subset=['Patient','Weeks'], keep = False, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## CHECK SUBMISSION FORMAT\nsubmissionDataFrame = pd.read_csv(\"../input/osic-pulmonary-fibrosis-progression/sample_submission.csv\")\nprint(f\"The sample submission contains: {submissionDataFrame.shape[0]} rows and {submissionDataFrame.shape[1]} columns.\")\n# split Patient_Week Column and re-arrage columns\nsubmissionDataFrame[['Patient','Weeks']] = submissionDataFrame.Patient_Week.str.split(\"_\",expand = True)\nsubmissionDataFrame =  submissionDataFrame[['Patient','Weeks','Confidence', 'Patient_Week']]\nsubmissionDataFrame = submissionDataFrame.merge(testDataFrame.drop('Weeks', axis = 1), on = \"Patient\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# introduce a column to indicate the source (train/test) for the data\ntrainDataFrame['Source'] = 'train'\nsubmissionDataFrame['Source'] = 'test'\ndataFrame = trainDataFrame.append([submissionDataFrame])\ndataFrame.reset_index(inplace = True)\ndataFrame.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_week(df):\n    # make a copy to not change original df    \n    copy = df.copy()\n    # ensure all Weeks values are INT and not accidentaly saved as string\n    copy['Weeks'] = copy['Weeks'].astype(int)\n    # as test data is containing all weeks, \n    copy.loc[copy.Source == 'test','min_week'] = np.nan\n    copy[\"min_week\"] = copy.groupby('Patient')['Weeks'].transform('min')\n    copy['baselined_week'] = copy['Weeks'] - copy['min_week']\n    \n    return copy  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_base_FVC(df):\n    # same as above\n    copy = df.copy()\n    base = copy.loc[copy.Weeks == copy.min_week]\n    base = base[['Patient','FVC']].copy()\n    base.columns = ['Patient','base_FVC']\n    \n    # add a row which contains the cumulated sum of rows for each patient\n    base['nb'] = 1\n    base['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\n    \n    # drop all except the first row for each patient (=unique rows!), containing the min_week\n    base = base[base.nb == 1]\n    base.drop('nb', axis = 1, inplace = True)\n    \n    # merge the rows containing the base_FVC on the original _df\n    copy = copy.merge(base, on = 'Patient', how = 'left')    \n    copy.drop(['min_week'], axis = 1)\n    \n    return copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataFrame = get_week(dataFrame)\ndataFrame = get_base_FVC(dataFrame)\ndataFrame.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.preprocessing import OneHotEncoder\n#from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n#from sklearn.compose import ColumnTransformer\n\n# define which attributes shall not be transformed, are numeric or categorical\nnot_to_transform_attribute = ['Patient', 'Weeks', 'min_week']\ntransform_Attributes = ['FVC', 'Percent', 'Age', 'baselined_week', 'base_FVC']\ncategoricalFeature = ['Sex', 'SmokingStatus']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def own_MinMaxColumnScaler(df, columns):\n    \"\"\"Adds columns with scaled numeric values to range [0, 1]\n    using the formula X_scld = (X - X.min) / (X.max - X.min)\"\"\"\n    for col in columns:\n        new_col_name = col + '_scld'\n        col_min = df[col].min()\n        col_max = df[col].max()        \n        df[new_col_name] = (df[col] - col_min) / ( col_max - col_min )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def own_OneHotColumnCreator(df, columns):\n    \"\"\"OneHot Encodes categorical features. Adds a column for each unique value per column\"\"\"\n    for col in cat_attribs:\n        for value in df[col].unique():\n            df[value] = (df[col] == value).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## APPLY DEFINED TRANSFORMATIONS\nown_MinMaxColumnScaler(dataFrame, transform_Attributes)\nown_OneHotColumnCreator(dataFrame, categoricalFeature)\n\n#data_df[data_df.Source != \"train\"].head()\n#data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDataFrame = dataFrame.loc[dataFrame.Source == 'train']\nSubmission = dataFrame.loc[dataFrame.Source == 'test']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"featureList = ['baselined_week_scld', 'Percent_scld', 'Age_scld', 'base_FVC_scld', 'Male', 'Female', 'Ex-smoker', 'Never smoked', 'Currently smokes']\n#EPOCHS = 1000\nEPOCHS = 100\n#BATCH_SIZE = 128\nBATCH_SIZE = 200\n## LOSS; set tradeoff btw. Pinball-loss and adding score\n_lambda = 0.8 # 0.8 default\n## Optimizers\nADAM = tf.keras.optimizers.Adam(lr = 0.1,beta_1 = 0.9, beta_2 = 0.999,decay = 0.01)\nSGD = tf.keras.optimizers.SGD()\n# choose ADAM or SGD\noptimizer = ADAM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create constants for the loss function\nC1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n\n# define competition metric\ndef score(y_true, y_pred):\n    \"\"\"Calculate the competition metric\"\"\"\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype = tf.float32) )\n    metric = (delta / sigma_clip) * sq2 + tf.math.log(sigma_clip * sq2)\n    return K.mean(metric)\n\n# define pinball loss\ndef qloss(y_true, y_pred):\n    \"\"\"Calculate Pinball loss\"\"\"\n    # IMPORTANT: define quartiles, feel free to change here!\n    qs = [0.2, 0.50, 0.8]\n    q = tf.constant(np.array([qs]), dtype = tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q * e, (q-1) * e)\n    return K.mean(v)\n\n# combine competition metric and pinball loss to a joint loss function\ndef mloss(_lambda):\n    \"\"\"Combine Score and qloss\"\"\"\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda) * score(y_true, y_pred)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    \"Creates and returns a model\"\n    inp = Layers.Input((len(featureList),), name = \"Patient\")\n    x = Layers.Dense(128, activation = \"relu\", name = \"d1\")(inp)\n    x = Layers.Dropout(0.25)(x)\n    x = Layers.Dense(128, activation = \"relu\", name = \"d2\")(x)\n    x = Layers.Dropout(0.2)(x)\n    # predicting the \n    p1 = Layers.Dense(3, activation = \"relu\", name = \"p1\")(x)\n    # quantile adjusting p1 predictions\n    p2 = Layers.Dense(3, activation = \"relu\", name = \"p2\")(x)\n    preds = Layers.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis = 1), \n                     name = \"preds\")([p1, p2])\n    \n    model = Models.Model(inp, preds, name = \"NeuralNet\")\n    model.compile(loss = mloss(_lambda), optimizer = optimizer, metrics = [score])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neuralNet = get_model()\nneuralNet.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get target value\ny = trainDataFrame['FVC'].values\ny=y.astype(float)\n# get training & test data\nX_train = trainDataFrame[featureList].values\nX_test = Submission[featureList].values\n\n# instantiate target arrays\ntrain_preds = np.zeros((X_train.shape[0], 3))\ntest_preds = np.zeros((X_test.shape[0], 3))\nprint(y.dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NFOLD = 10\nNFOLD = 5\nkf = KFold(n_splits = NFOLD)\n\ncount = 0\nfor train_idx, val_idx in kf.split(X_train):\n    count += 1\n    print(f\"FOLD {count}:\")\n    \n    # create and fit model\n    net = get_model()\n    net.fit(X_train[train_idx], y[train_idx], batch_size = BATCH_SIZE, epochs = EPOCHS, verbose = 1 ,validation_data = (X_train[val_idx], y[val_idx]), shuffle = True , workers = 3) \n    \n    # evaluate\n    print(\"Train:\", net.evaluate(X_train[train_idx], y[train_idx], verbose = 1, batch_size = BATCH_SIZE))\n    print(\"Val:\", net.evaluate(X_train[val_idx], y[val_idx], verbose = 0, batch_size = BATCH_SIZE))\n    \n    # generate predictions for the known train data and the unknown test data\n    train_preds[val_idx] = net.predict(X_train[val_idx], batch_size = BATCH_SIZE, verbose = 0)\n    \n    print(\"Predicting Test...\")\n    test_preds += net.predict(X_test, batch_size = BATCH_SIZE, verbose = 0) / NFOLD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sigma_opt = mean_absolute_error(y, train_preds[:,1])\nsigma_uncertain = train_preds[:,2] - train_preds[:,0]\nsigma_mean = np.mean(sigma_uncertain)\nprint(sigma_opt, sigma_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Submission['FVC1'] = test_preds[:, 1]\nSubmission['Confidence1'] = test_preds[:,2] - test_preds[:,0]\n\n# get rid of unused data and show some non-empty data\nsubmission = Submission[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()\nsubmission.loc[~submission.FVC1.isnull()].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.loc[~submission.FVC1.isnull(),'FVC'] = submission.loc[~submission.FVC1.isnull(),'FVC1']\n\nif sigma_mean < 70:\n    submission['Confidence'] = sigma_opt\nelse:\n    submission.loc[~submission.FVC1.isnull(),'Confidence'] = submission.loc[~submission.FVC1.isnull(),'Confidence1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"org_test = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv')\n\nfor i in range(len(org_test)):\n    submission.loc[submission['Patient_Week']==org_test.Patient[i]+'_'+str(org_test.Weeks[i]), 'FVC'] = org_test.FVC[i]\n    submission.loc[submission['Patient_Week']==org_test.Patient[i]+'_'+str(org_test.Weeks[i]), 'Confidence'] = 70","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"/kaggle/working/submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}